{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5500_assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0Mqcz1isqo_"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/assign1_data.csv\"\n",
        "data = np.genfromtxt(filename, dtype='float', delimiter=',', skip_header=1)\n",
        "X, y = data[ : , :-1], data[ : , -1].astype(int)\n",
        "X_train, y_train = X[:400], y[:400]\n",
        "X_test, y_test = X[400:], y[400:]"
      ],
      "metadata": {
        "id": "MDL4C8HKsxA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8VQz54PIxSf",
        "outputId": "66f6653c-7d7a-4d38-f42f-24e04d208300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.39764,  0.53117,  1.64858,  0.     ],\n",
              "       [-2.16148,  0.26983, -2.86461,  1.     ],\n",
              "       [-1.70622,  2.23025, -1.64642,  2.     ],\n",
              "       ...,\n",
              "       [ 2.49018,  0.61979,  1.61818,  0.     ],\n",
              "       [ 1.58315,  0.38558,  2.2053 ,  0.     ],\n",
              "       [ 0.21345,  2.0098 ,  3.12069,  0.     ]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        \"\"\"\n",
        "        Initialize weights & biases.\n",
        "        Weights should be initialized with values drawn from a normal\n",
        "        distribution scaled by 0.01.\n",
        "        Biases are initialized to 0.0.\n",
        "        \"\"\"\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1,n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        A forward pass through the layer to give z.\n",
        "        Compute it using np.dot(...) and then add the biases.\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        self.z = np.dot(self.inputs, self.weights) + self.biases\n",
        "    def backward(self, dz):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        \"\"\"\n",
        "        # Gradients of weights\n",
        "        self.dweights = np.dot(self.inputs.T, dz)\n",
        "        # Gradients of biases\n",
        "        self.dbiases = np.sum(dz, axis=0, keepdims=True)\n",
        "        # Gradients of inputs\n",
        "        self.dinputs = np.dot(dz, self.weights.T)"
      ],
      "metadata": {
        "id": "JHMcYF3XttOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLu\n",
        "class ReLu:\n",
        "    \"\"\"\n",
        "    ReLu activation\n",
        "    \"\"\"\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        \"\"\"\n",
        "        self.z = z\n",
        "        self.activity = np.maximum(0,self.z)\n",
        "    def backward(self, dactivity):\n",
        "        \"\"\"\n",
        "        Backward pass\n",
        "        \"\"\"\n",
        "        self.dz = dactivity.copy()\n",
        "        self.dz[self.z <= 0] = 0.0\n",
        "\n",
        "# Softmax\n",
        "class Softmax:\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        self.probs = e_z / e_z.sum(axis=1, keepdims=True)\n",
        "        return self.probs\n",
        "    def backward(self, dprobs):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        # Empty array\n",
        "        self.dz = np.empty_like(dprobs)\n",
        "        for i, (prob, dprob) in enumerate(zip(self.probs, dprobs)):\n",
        "            # flatten to a column vector\n",
        "            prob = prob.reshape(-1, 1)\n",
        "            # Jacobian matrix\n",
        "            jacobian = np.diagflat(prob) - np.dot(prob, prob.T)\n",
        "            self.dz[i] = np.dot(jacobian, dprob)"
      ],
      "metadata": {
        "id": "5MYCgj6kt7XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    def forward(self, probs, oh_y_true):\n",
        "        \"\"\"\n",
        "        Use one-hot encoded y_true.\n",
        "        \"\"\"\n",
        "        # clip to prevent division by 0\n",
        "        # clip both sides to not bias up.\n",
        "        probs_clipped = np.clip(probs, 1e-7, 1 - 1e-7)\n",
        "        # negative log likelihoods\n",
        "        loss = -np.sum(oh_y_true * np.log(probs_clipped), axis=1)\n",
        "        return loss.mean(axis=0)\n",
        "    def backward(self, probs, oh_y_true):\n",
        "        \"\"\"\n",
        "        Use one-hot encoded y_true.\n",
        "        \"\"\"\n",
        "        # Number of examples in batch and number of classes\n",
        "        batch_sz, n_class = probs.shape\n",
        "        # get the gradient\n",
        "        self.dprobs = -oh_y_true / probs\n",
        "        # normalize the gradient\n",
        "        self.dprobs = self.dprobs / batch_sz"
      ],
      "metadata": {
        "id": "UtPnZBYOui_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=1.0):\n",
        "        # Initialize the optimizer with a learning rate\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def update_params(self, layer):\n",
        "        layer.weights = layer.weights - self.learning_rate * layer.dweights\n",
        "        layer.biases = layer.biases - self.learning_rate * layer.dbiases"
      ],
      "metadata": {
        "id": "hrUuqjs4uqWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions(probs):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    y_preds = np.argmax(probs, axis=1)\n",
        "    return y_preds\n",
        "\n",
        "# Accuracy\n",
        "def accuracy(y_preds, y_true):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    return np.mean(y_preds == y_true)"
      ],
      "metadata": {
        "id": "gPtNM1NDusW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X, y_true, oh_y_true):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.z)\n",
        "  dense2.forward(activation1.activity)\n",
        "  activation2.forward(dense2.z)\n",
        "  dense3.forward(activation2.activity)\n",
        "  probs = output_activation.forward(dense3.z)\n",
        "  loss = crossentropy.forward(probs,oh_y_true)\n",
        "  return probs, loss"
      ],
      "metadata": {
        "id": "3_gncD4p3h5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(probs, y_true, oh_y_true):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  crossentropy.backward(probs,oh_y_true)\n",
        "  output_activation.backward(crossentropy.dprobs)\n",
        "  dense3.backward(output_activation.dz)\n",
        "  activation2.backward(dense3.dinputs)\n",
        "  dense2.backward(activation2.dz)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dz)"
      ],
      "metadata": {
        "id": "u0vfADbj5KaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_sz = 20\n",
        "n_batch = int(len(X_train)/batch_sz)\n",
        "n_class = 3\n",
        "\n",
        "dense1 = DenseLayer(3,4)\n",
        "dense2 = DenseLayer(4,8)\n",
        "dense3 = DenseLayer(8,3)\n",
        "activation1 = ReLu()\n",
        "activation2 = ReLu()\n",
        "output_activation = Softmax()\n",
        "crossentropy = CrossEntropyLoss()\n",
        "optimizer = SGD()"
      ],
      "metadata": {
        "id": "f-ESr8CB5M55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  print('epoch:', epoch)\n",
        "  for batch_i in range(n_batch):\n",
        "    x = np.split(X_train, n_batch)[batch_i]\n",
        "    y_true = np.split(y_train, n_batch)[batch_i]\n",
        "    oh_y_true = np.eye(n_class)[y_true]\n",
        "    forward_pass(x, y_true, oh_y_true)\n",
        "    probs, loss = forward_pass(x, y_true, oh_y_true)\n",
        "    y_preds = predictions(probs)\n",
        "    print('Accuracy: ', accuracy(y_preds, y_true), 'Loss: ', loss)\n",
        "    backward_pass(probs, y_true, oh_y_true)\n",
        "    optimizer.update_params(dense3)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.update_params(dense1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-Syf_TKkia",
        "outputId": "329bcac0-0a76-4ae7-ed3f-23c467502467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Accuracy:  0.35 Loss:  1.0986118146258925\n",
            "Accuracy:  0.3 Loss:  1.1030625214094019\n",
            "Accuracy:  0.4 Loss:  1.0916947849735963\n",
            "Accuracy:  0.35 Loss:  1.0965964089252\n",
            "Accuracy:  0.25 Loss:  1.1009704270016252\n",
            "Accuracy:  0.2 Loss:  1.1118231715459848\n",
            "Accuracy:  0.5 Loss:  1.0613096808440008\n",
            "Accuracy:  0.25 Loss:  1.174047897267075\n",
            "Accuracy:  0.3 Loss:  1.1145358199768707\n",
            "Accuracy:  0.1 Loss:  1.104132035986248\n",
            "Accuracy:  0.25 Loss:  1.1729371705691531\n",
            "Accuracy:  0.5 Loss:  1.1103061151009175\n",
            "Accuracy:  0.2 Loss:  1.1603307153053781\n",
            "Accuracy:  0.4 Loss:  1.0797080586427183\n",
            "Accuracy:  0.15 Loss:  1.1806473290943678\n",
            "Accuracy:  0.35 Loss:  1.1144720573036715\n",
            "Accuracy:  0.6 Loss:  1.0924789463941253\n",
            "Accuracy:  0.25 Loss:  1.1740570901405853\n",
            "Accuracy:  0.25 Loss:  1.086717279270005\n",
            "Accuracy:  0.35 Loss:  1.1174486233506298\n",
            "epoch: 1\n",
            "Accuracy:  0.3 Loss:  1.1220096060617795\n",
            "Accuracy:  0.3 Loss:  1.1054155259344993\n",
            "Accuracy:  0.4 Loss:  1.0997323260035112\n",
            "Accuracy:  0.35 Loss:  1.0973205097491883\n",
            "Accuracy:  0.25 Loss:  1.1036882170401872\n",
            "Accuracy:  0.2 Loss:  1.1132201737558538\n",
            "Accuracy:  0.5 Loss:  1.059313748014138\n",
            "Accuracy:  0.25 Loss:  1.174735116118367\n",
            "Accuracy:  0.3 Loss:  1.1138917359571952\n",
            "Accuracy:  0.1 Loss:  1.1050729734707845\n",
            "Accuracy:  0.25 Loss:  1.1729730483345775\n",
            "Accuracy:  0.5 Loss:  1.110206895437545\n",
            "Accuracy:  0.2 Loss:  1.1596260082327254\n",
            "Accuracy:  0.4 Loss:  1.0796582679184532\n",
            "Accuracy:  0.15 Loss:  1.180897498228962\n",
            "Accuracy:  0.35 Loss:  1.114338447978615\n",
            "Accuracy:  0.6 Loss:  1.0925799514783157\n",
            "Accuracy:  0.25 Loss:  1.173991752639288\n",
            "Accuracy:  0.25 Loss:  1.0866518172568802\n",
            "Accuracy:  0.35 Loss:  1.1174476769174926\n",
            "epoch: 2\n",
            "Accuracy:  0.3 Loss:  1.1220105320928775\n",
            "Accuracy:  0.3 Loss:  1.1054079400454933\n",
            "Accuracy:  0.4 Loss:  1.0997232475716694\n",
            "Accuracy:  0.35 Loss:  1.0973042839869593\n",
            "Accuracy:  0.25 Loss:  1.1036792267432505\n",
            "Accuracy:  0.2 Loss:  1.1132091845483516\n",
            "Accuracy:  0.5 Loss:  1.0592906780710336\n",
            "Accuracy:  0.25 Loss:  1.174738489331193\n",
            "Accuracy:  0.3 Loss:  1.1138683868528638\n",
            "Accuracy:  0.1 Loss:  1.1050422067985577\n",
            "Accuracy:  0.25 Loss:  1.1729940104379828\n",
            "Accuracy:  0.5 Loss:  1.110192516385551\n",
            "Accuracy:  0.2 Loss:  1.1596127111285803\n",
            "Accuracy:  0.4 Loss:  1.0796297942300468\n",
            "Accuracy:  0.15 Loss:  1.1808629611921782\n",
            "Accuracy:  0.35 Loss:  1.1143211243028284\n",
            "Accuracy:  0.6 Loss:  1.0924950873347108\n",
            "Accuracy:  0.25 Loss:  1.1740501216356918\n",
            "Accuracy:  0.25 Loss:  1.08661043252412\n",
            "Accuracy:  0.35 Loss:  1.1174140307463098\n",
            "epoch: 3\n",
            "Accuracy:  0.3 Loss:  1.1219750101109667\n",
            "Accuracy:  0.3 Loss:  1.1053703960474244\n",
            "Accuracy:  0.4 Loss:  1.0996607971595704\n",
            "Accuracy:  0.35 Loss:  1.097236414189789\n",
            "Accuracy:  0.25 Loss:  1.1036434748539894\n",
            "Accuracy:  0.2 Loss:  1.1131552760942112\n",
            "Accuracy:  0.5 Loss:  1.0591828163410049\n",
            "Accuracy:  0.25 Loss:  1.174765492533987\n",
            "Accuracy:  0.3 Loss:  1.1137556191608478\n",
            "Accuracy:  0.1 Loss:  1.1048812268570711\n",
            "Accuracy:  0.25 Loss:  1.173103726064342\n",
            "Accuracy:  0.5 Loss:  1.1101153781685809\n",
            "Accuracy:  0.2 Loss:  1.1595514998160654\n",
            "Accuracy:  0.4 Loss:  1.0795063242107135\n",
            "Accuracy:  0.15 Loss:  1.1807402941794327\n",
            "Accuracy:  0.35 Loss:  1.1141669500140532\n",
            "Accuracy:  0.6 Loss:  1.092018414107457\n",
            "Accuracy:  0.25 Loss:  1.1742310295926424\n",
            "Accuracy:  0.25 Loss:  1.0863762189873256\n",
            "Accuracy:  0.35 Loss:  1.1171451819843499\n",
            "epoch: 4\n",
            "Accuracy:  0.3 Loss:  1.1216588880996163\n",
            "Accuracy:  0.3 Loss:  1.105019311402876\n",
            "Accuracy:  0.4 Loss:  1.0990710237607337\n",
            "Accuracy:  0.35 Loss:  1.0965357234052433\n",
            "Accuracy:  0.25 Loss:  1.1032198434577658\n",
            "Accuracy:  0.2 Loss:  1.1122742960547178\n",
            "Accuracy:  0.5 Loss:  1.0576867580691274\n",
            "Accuracy:  0.25 Loss:  1.1745043072520427\n",
            "Accuracy:  0.3 Loss:  1.1118645896161419\n",
            "Accuracy:  0.1 Loss:  1.1025340050987782\n",
            "Accuracy:  0.25 Loss:  1.1729603197170753\n",
            "Accuracy:  0.55 Loss:  1.1088708946204153\n",
            "Accuracy:  0.2 Loss:  1.1574431144038697\n",
            "Accuracy:  0.4 Loss:  1.0756264845733128\n",
            "Accuracy:  0.15 Loss:  1.1724671688062593\n",
            "Accuracy:  0.35 Loss:  1.1035935991263826\n",
            "Accuracy:  0.6 Loss:  1.0570332788521686\n",
            "Accuracy:  0.25 Loss:  1.1663540887897927\n",
            "Accuracy:  0.25 Loss:  1.0520075301707148\n",
            "Accuracy:  0.45 Loss:  1.0526437748222697\n",
            "epoch: 5\n",
            "Accuracy:  0.55 Loss:  1.0065792454406537\n",
            "Accuracy:  0.45 Loss:  0.8799015633615241\n",
            "Accuracy:  0.65 Loss:  0.6932423190198795\n",
            "Accuracy:  0.7 Loss:  0.7422747083133567\n",
            "Accuracy:  0.7 Loss:  0.8262099927959685\n",
            "Accuracy:  0.8 Loss:  0.5256071103158001\n",
            "Accuracy:  0.65 Loss:  0.7597385042586803\n",
            "Accuracy:  0.75 Loss:  0.6623418740847199\n",
            "Accuracy:  0.65 Loss:  0.6637959895159817\n",
            "Accuracy:  0.7 Loss:  0.7050178105764999\n",
            "Accuracy:  0.65 Loss:  0.6305978162289937\n",
            "Accuracy:  0.6 Loss:  1.1395439768117934\n",
            "Accuracy:  0.3 Loss:  0.969351488001186\n",
            "Accuracy:  0.7 Loss:  0.7028447857696991\n",
            "Accuracy:  0.55 Loss:  0.8604340251756218\n",
            "Accuracy:  0.55 Loss:  0.8445345481420705\n",
            "Accuracy:  0.65 Loss:  0.6374070874250984\n",
            "Accuracy:  0.55 Loss:  0.6953223990652405\n",
            "Accuracy:  0.5 Loss:  0.6305927705768676\n",
            "Accuracy:  0.8 Loss:  0.6205801232625703\n",
            "epoch: 6\n",
            "Accuracy:  0.95 Loss:  0.4537376934919727\n",
            "Accuracy:  0.75 Loss:  0.7142024005728429\n",
            "Accuracy:  0.9 Loss:  0.5041865287899301\n",
            "Accuracy:  0.85 Loss:  0.42442681545669114\n",
            "Accuracy:  0.95 Loss:  0.277142696611833\n",
            "Accuracy:  0.9 Loss:  0.26241565646047793\n",
            "Accuracy:  0.85 Loss:  0.4579223244414997\n",
            "Accuracy:  1.0 Loss:  0.16167495752944963\n",
            "Accuracy:  0.9 Loss:  0.279749491707609\n",
            "Accuracy:  0.95 Loss:  0.16459666308814974\n",
            "Accuracy:  0.75 Loss:  0.3584868611041202\n",
            "Accuracy:  0.8 Loss:  0.5977588626362326\n",
            "Accuracy:  0.8 Loss:  0.3962447049906429\n",
            "Accuracy:  1.0 Loss:  0.11558009346189453\n",
            "Accuracy:  0.9 Loss:  0.3699869010135544\n",
            "Accuracy:  0.95 Loss:  0.26006216749473327\n",
            "Accuracy:  1.0 Loss:  0.08068690735485681\n",
            "Accuracy:  1.0 Loss:  0.23644750694877872\n",
            "Accuracy:  0.9 Loss:  0.4165298404658374\n",
            "Accuracy:  0.9 Loss:  0.3449886849621699\n",
            "epoch: 7\n",
            "Accuracy:  0.9 Loss:  0.19242608421136367\n",
            "Accuracy:  0.8 Loss:  0.5231934825517442\n",
            "Accuracy:  0.9 Loss:  0.20679924719042714\n",
            "Accuracy:  0.9 Loss:  0.2547819638074382\n",
            "Accuracy:  0.95 Loss:  0.1643196414996966\n",
            "Accuracy:  0.9 Loss:  0.22120535736356062\n",
            "Accuracy:  0.85 Loss:  0.35753294922454787\n",
            "Accuracy:  1.0 Loss:  0.06407518771774379\n",
            "Accuracy:  0.9 Loss:  0.2852121101496673\n",
            "Accuracy:  0.95 Loss:  0.10404466369138668\n",
            "Accuracy:  0.75 Loss:  0.39999023611700685\n",
            "Accuracy:  0.75 Loss:  0.7542272206675069\n",
            "Accuracy:  0.8 Loss:  0.3831539975820027\n",
            "Accuracy:  1.0 Loss:  0.06572856831442898\n",
            "Accuracy:  0.9 Loss:  0.3755676180225253\n",
            "Accuracy:  0.95 Loss:  0.19211975392882832\n",
            "Accuracy:  1.0 Loss:  0.04963212032697131\n",
            "Accuracy:  0.95 Loss:  0.1866581138149335\n",
            "Accuracy:  0.9 Loss:  0.46254898780942416\n",
            "Accuracy:  0.9 Loss:  0.3165669594877689\n",
            "epoch: 8\n",
            "Accuracy:  0.9 Loss:  0.2110839168049586\n",
            "Accuracy:  0.85 Loss:  0.47198043696738967\n",
            "Accuracy:  0.85 Loss:  0.23351412931515605\n",
            "Accuracy:  0.9 Loss:  0.25467369606444973\n",
            "Accuracy:  1.0 Loss:  0.09787456561799177\n",
            "Accuracy:  0.85 Loss:  0.2733241663923198\n",
            "Accuracy:  0.85 Loss:  0.6367523287071468\n",
            "Accuracy:  1.0 Loss:  0.04927026237010945\n",
            "Accuracy:  0.9 Loss:  0.29730624235574216\n",
            "Accuracy:  0.95 Loss:  0.0925695037580802\n",
            "Accuracy:  0.8 Loss:  0.4347630727648105\n",
            "Accuracy:  0.75 Loss:  0.7215524198425618\n",
            "Accuracy:  0.8 Loss:  0.36462643006082357\n",
            "Accuracy:  1.0 Loss:  0.05685413516471136\n",
            "Accuracy:  0.9 Loss:  0.35699637545767066\n",
            "Accuracy:  0.95 Loss:  0.17628149464558848\n",
            "Accuracy:  1.0 Loss:  0.049218439460621856\n",
            "Accuracy:  0.95 Loss:  0.16283065128678675\n",
            "Accuracy:  0.9 Loss:  0.49445433679171913\n",
            "Accuracy:  0.9 Loss:  0.3127508978981162\n",
            "epoch: 9\n",
            "Accuracy:  0.9 Loss:  0.1841255190320197\n",
            "Accuracy:  0.85 Loss:  0.4478274211090885\n",
            "Accuracy:  0.9 Loss:  0.15714531412661642\n",
            "Accuracy:  0.9 Loss:  0.23559635914631621\n",
            "Accuracy:  1.0 Loss:  0.10838098129435485\n",
            "Accuracy:  0.85 Loss:  0.23466020440651456\n",
            "Accuracy:  0.85 Loss:  0.4160944654788753\n",
            "Accuracy:  1.0 Loss:  0.04247991675901809\n",
            "Accuracy:  0.9 Loss:  0.31100225955056254\n",
            "Accuracy:  0.95 Loss:  0.08208884097458177\n",
            "Accuracy:  0.8 Loss:  0.4337875084493706\n",
            "Accuracy:  0.8 Loss:  0.6901742543344754\n",
            "Accuracy:  0.8 Loss:  0.3391303169202595\n",
            "Accuracy:  1.0 Loss:  0.05083387496114984\n",
            "Accuracy:  0.9 Loss:  0.3736633641485634\n",
            "Accuracy:  0.95 Loss:  0.18384634706206932\n",
            "Accuracy:  1.0 Loss:  0.04170243715993727\n",
            "Accuracy:  0.95 Loss:  0.1591424353467249\n",
            "Accuracy:  0.9 Loss:  0.5129063135201838\n",
            "Accuracy:  0.9 Loss:  0.2949694312790773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs,pred_loss = forward_pass(X_test,y_test,np.eye(n_class)[y_test])\n",
        "pred_y = predictions(pred_probs)\n",
        "print(accuracy(pred_y,y_test))"
      ],
      "metadata": {
        "id": "k3hS5EGhAqmf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456ad4ba-335d-4049-bcae-2d1b50bd0742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n"
          ]
        }
      ]
    }
  ]
}